# Epochs, hidden_size are for both translate.py and train.py
epochs: 1
batch_size: 64
# model can be 'ct', 'cat' or 'mpctm'
model: 'cat'
device_id: 0
random_seed: 31415
embedding_dim: 300
learning_rate: 0.0005
hidden_size: 200
dense_dim: 200
output_dim: 100
num_layers_lstm: 2
use_cuda: False
use_softmax_classifier: False
use_bin: False
use_bidirectional: True
use_adam: True
use_parallel: False
save_path: 'saved_models'
# dataset can be "conala" or "codesearchnet or cs105"
dataset: 'codesearchnet'
# encoder can be 'LSTM' or 'Transformer'
encoder: 'LSTM'
full_bimpm: True
save_every: 1
negative_examples: 5
# Direction of Translation: code2lang or lang2code
translate_task: 'code2lang'
tune_thres: True